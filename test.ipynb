{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c99603d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV mean accuracies:\n",
      "  w=0.5: 0.7533\n",
      "  w=1.0: 0.7533\n",
      "  w=2.0: 0.7533\n",
      "  w=5.0: 0.7533\n",
      "Best w: 0.5\n",
      "Example predictions: [2 0 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "from typing import Callable, Iterable, Any, Dict, Tuple\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score\n",
    "\n",
    "def tune_and_train_1nn(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    metric_func: Callable[[np.ndarray, np.ndarray, Any], float],\n",
    "    w_candidates: Iterable[Any],\n",
    "    k: int = 5,\n",
    "    stratify: bool = True,\n",
    "    random_state: int | None = None,\n",
    "    n_jobs: int = 1,\n",
    ") -> Tuple[KNeighborsClassifier, Any, Dict[Any, float]]:\n",
    "    \"\"\"\n",
    "    Tune hyperparameter w for a custom distance metric d(a,b,w) using k-fold CV,\n",
    "    then fit a 1-NN classifier with the chosen w.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : (n_samples, n_features) array\n",
    "    y : (n_samples,) array of labels\n",
    "    metric_func : callable(a, b, w) -> float\n",
    "        Black-box distance between two 1-D vectors a and b, using hyperparam w.\n",
    "    w_candidates : iterable of candidate w values\n",
    "    k : number of folds for CV (default 5)\n",
    "    stratify : whether to use StratifiedKFold when possible\n",
    "    random_state : int or None for CV shuffling\n",
    "    n_jobs : number of jobs for CV (set to 1 to avoid pickling issues with some callables)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    clf_best : fitted KNeighborsClassifier (n_neighbors=1) using best w\n",
    "    best_w : chosen hyperparameter\n",
    "    cv_results : dict mapping w -> mean CV accuracy\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(\"X must be 2D: (n_samples, n_features)\")\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"X and y must have same number of samples\")\n",
    "\n",
    "    # choose CV splitter\n",
    "    if stratify and len(np.unique(y)) > 1:\n",
    "        cv_splitter = StratifiedKFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "    else:\n",
    "        cv_splitter = KFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "\n",
    "    cv_results = {}\n",
    "    # IMPORTANT: use n_jobs=1 here by default because cross_val_score may pickle the callable metric.\n",
    "    # If your metric and environment are picklable, you can set n_jobs > 1.\n",
    "    for w in w_candidates:\n",
    "        # create a callable metric that binds w\n",
    "        bound_metric = lambda a, b, w=w: metric_func(a, b, w)\n",
    "        # KNeighborsClassifier accepts a callable metric; this will use brute-force search\n",
    "        clf = KNeighborsClassifier(n_neighbors=1, metric=bound_metric, algorithm='brute')\n",
    "        scores = cross_val_score(clf, X, y, cv=cv_splitter, scoring='accuracy', n_jobs=n_jobs)\n",
    "        cv_results[w] = float(np.mean(scores))\n",
    "\n",
    "    # pick best w (max mean accuracy). tie -> first encountered in w_candidates\n",
    "    best_w = max(w_candidates, key=lambda w: cv_results[w])\n",
    "\n",
    "    # fit final classifier on entire data with the chosen w\n",
    "    best_metric = lambda a, b, w=best_w: metric_func(a, b, w)\n",
    "    clf_best = KNeighborsClassifier(n_neighbors=1, metric=best_metric, algorithm='brute')\n",
    "    clf_best.fit(X, y)\n",
    "\n",
    "    return clf_best, best_w, cv_results\n",
    "\n",
    "# -----------------------\n",
    "# Example usage\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.datasets import make_classification\n",
    "\n",
    "    # example: weighted euclidean metric where w is a scalar weight or vector of per-feature weights\n",
    "    def weighted_euclidean(a, b, w):\n",
    "        a = np.asarray(a); b = np.asarray(b)\n",
    "        if np.isscalar(w):\n",
    "            wvec = np.full(a.shape, w)\n",
    "        else:\n",
    "            wvec = np.asarray(w)\n",
    "            if wvec.shape != a.shape:\n",
    "                raise ValueError(\"w must be scalar or same shape as vectors\")\n",
    "        diff = (a - b) * wvec\n",
    "        return np.linalg.norm(diff)\n",
    "\n",
    "    X, y = make_classification(n_samples=300, n_features=5, n_informative=3,\n",
    "                               n_redundant=0, n_classes=3, random_state=0)\n",
    "\n",
    "    w_candidates = [0.5, 1.0, 2.0, 5.0]\n",
    "    clf, best_w, results = tune_and_train_1nn(X, y, weighted_euclidean, w_candidates,\n",
    "                                              k=5, stratify=True, random_state=0, n_jobs=1)\n",
    "\n",
    "    print(\"CV mean accuracies:\")\n",
    "    for w, acc in results.items():\n",
    "        print(f\"  w={w}: {acc:.4f}\")\n",
    "    print(\"Best w:\", best_w)\n",
    "    print(\"Example predictions:\", clf.predict(X[:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af17805a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation of 5 candidates on 3 processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV results (w -> mean accuracy):\n",
      "  0.2: 0.7162\n",
      "  0.5: 0.7162\n",
      "  1.0: 0.7162\n",
      "  2.0: 0.7162\n",
      "  5.0: 0.7162\n",
      "Best w: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 200/200 [00:00<00:00, 467.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example predictions (first 10): [3 0 1 0 0 3 3 1 2 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Callable, Iterable, Any, Dict, Tuple, List\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# ---- module-level globals for worker processes (set via initializer) ----\n",
    "_METRIC_FUNC = None\n",
    "_X = None\n",
    "_Y = None\n",
    "_K = None\n",
    "_RANDOM_STATE = None\n",
    "\n",
    "def _worker_init(metric_func, X, y, k, random_state):\n",
    "    \"\"\"Initializer for worker processes: set globals once per worker.\"\"\"\n",
    "    global _METRIC_FUNC, _X, _Y, _K, _RANDOM_STATE\n",
    "    _METRIC_FUNC = metric_func\n",
    "    _X = X\n",
    "    _Y = y\n",
    "    _K = k\n",
    "    _RANDOM_STATE = random_state\n",
    "\n",
    "def _evaluate_w(w) -> Tuple[Any, float]:\n",
    "    \"\"\"\n",
    "    Worker: evaluate one w via KFold CV (plain KFold since labels are balanced).\n",
    "    Returns (w, mean_accuracy).\n",
    "    \"\"\"\n",
    "    global _METRIC_FUNC, _X, _Y, _K, _RANDOM_STATE\n",
    "    cv = KFold(n_splits=_K, shuffle=True, random_state=_RANDOM_STATE)\n",
    "    scores: List[float] = []\n",
    "    for train_idx, val_idx in cv.split(_X):\n",
    "        X_tr, y_tr = _X[train_idx], _Y[train_idx]\n",
    "        X_val, y_val = _X[val_idx], _Y[val_idx]\n",
    "\n",
    "        correct = 0\n",
    "        for xi, yi in zip(X_val, y_val):\n",
    "            # brute-force distances (metric must be picklable and top-level)\n",
    "            dists = [float(_METRIC_FUNC(xi, xt, w)) for xt in X_tr]\n",
    "            nn = int(np.argmin(dists))\n",
    "            if y_tr[nn] == yi:\n",
    "                correct += 1\n",
    "        scores.append(correct / len(y_val))\n",
    "    return (w, float(np.mean(scores)))\n",
    "\n",
    "def tune_and_train_1nn_parallel(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    metric_func: Callable[[np.ndarray, np.ndarray, Any], float],\n",
    "    w_candidates: Iterable[Any],\n",
    "    k: int = 5,\n",
    "    random_state: int | None = None,\n",
    "    n_procs: int = 8,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[KNeighborsClassifier, Any, Dict[Any, float]]:\n",
    "    \"\"\"\n",
    "    Parallel search over w_candidates using multiprocessing.Pool and KFold CV.\n",
    "    Returns: (fitted_1nn_classifier, best_w, cv_results_dict)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    w_list = list(w_candidates)\n",
    "    if len(w_list) == 0:\n",
    "        raise ValueError(\"w_candidates must be non-empty\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Starting evaluation of {len(w_list)} candidates on {n_procs} processes...\")\n",
    "\n",
    "    init_args = (metric_func, X, y, k, random_state)\n",
    "    with Pool(processes=n_procs, initializer=_worker_init, initargs=init_args) as pool:\n",
    "        results_iter = pool.imap(_evaluate_w, w_list)\n",
    "        results = list(tqdm(results_iter, total=len(w_list)))\n",
    "\n",
    "    cv_results: Dict[Any, float] = {w: acc for (w, acc) in results}\n",
    "    # pick best w (highest mean accuracy); tie -> first in w_list order\n",
    "    best_w = max(w_list, key=lambda w: cv_results[w])\n",
    "\n",
    "    # final classifier on full data using chosen best_w\n",
    "    def bound_metric(a, b, w=best_w):\n",
    "        return metric_func(a, b, w)\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=1, metric=bound_metric, algorithm='brute')\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"CV results (w -> mean accuracy):\")\n",
    "        for w in w_list:\n",
    "            print(f\"  {w}: {cv_results[w]:.4f}\")\n",
    "        print(\"Best w:\", best_w)\n",
    "\n",
    "    return clf, best_w, cv_results\n",
    "\n",
    "# -----------------------\n",
    "# Example usage\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.datasets import make_classification\n",
    "\n",
    "    # Example metric (top-level function so it's picklable)\n",
    "    def weighted_euclidean(a, b, w):\n",
    "        a = np.asarray(a); b = np.asarray(b)\n",
    "        if np.isscalar(w):\n",
    "            wvec = np.full(a.shape, float(w))\n",
    "        else:\n",
    "            wvec = np.asarray(w, dtype=float)\n",
    "            if wvec.shape != a.shape:\n",
    "                raise ValueError(\"w must be scalar or same shape as input vectors\")\n",
    "        diff = (a - b) * wvec\n",
    "        return float(np.linalg.norm(diff))\n",
    "\n",
    "    # Synthetic balanced dataset\n",
    "    X, y = make_classification(n_samples=800, n_features=8, n_informative=6,\n",
    "                               n_redundant=0, n_classes=4, weights=None, random_state=0)\n",
    "\n",
    "    w_candidates = [0.2, 0.5, 1.0, 2.0, 5.0]\n",
    "    clf, best_w, cv_results = tune_and_train_1nn_parallel(\n",
    "        X, y, weighted_euclidean, w_candidates,\n",
    "        k=5, random_state=0, n_procs=3, verbose=True\n",
    "    )\n",
    "\n",
    "    # Optional: parallel predictions per-sample using the same imap pattern\n",
    "    # (keeps your syntax: with Pool(...); pool.imap(...))\n",
    "    def _pred_worker_init(classifier):\n",
    "        global _PRED_CLF\n",
    "        _PRED_CLF = classifier\n",
    "\n",
    "    def _predict_single(x):\n",
    "        global _PRED_CLF\n",
    "        return int(_PRED_CLF.predict([x])[0])\n",
    "\n",
    "    X_test = X[:200]\n",
    "    with Pool(processes=3, initializer=_pred_worker_init, initargs=(clf,)) as pool:\n",
    "        y_pred_list = list(tqdm(pool.imap(_predict_single, X_test), total=len(X_test)))\n",
    "    y_pred = np.array(y_pred_list)\n",
    "\n",
    "    print(\"Example predictions (first 10):\", y_pred[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2baafa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :  [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[  0   1   2   3   4   5   6   9  10  11  12  13  14  15  17  18  19  20\n",
      "  21  23  25  27  28  29  30  31  32  34  35  36  38  39  41  42  43  46\n",
      "  47  48  49  50  52  53  55  56  57  58  59  60  61  64  65  67  68  69\n",
      "  70  72  74  75  77  79  80  81  82  83  84  85  87  88  89  91  92  94\n",
      "  95  96  98  99 101 102 103 104 105 106 108 109 110 111 112 113 115 116\n",
      " 117 118 119 120 122 123 124 125 127 128 129 130 131 132 133 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "[  0   1   3   4   5   6   7   8   9  11  12  13  14  15  16  17  19  20\n",
      "  21  22  23  24  25  26  28  29  30  31  32  33  34  35  36  37  38  39\n",
      "  40  41  42  44  45  46  47  48  49  51  52  53  54  55  57  58  62  63\n",
      "  64  65  66  67  68  70  71  72  73  74  75  76  77  78  79  81  82  85\n",
      "  86  87  88  89  90  91  93  94  95  96  97  98  99 100 101 102 103 104\n",
      " 105 107 109 110 111 113 114 115 117 118 120 121 122 124 125 126 128 129\n",
      " 130 131 134 136 138 139 140 142 143 145 148 149]\n",
      "[  0   1   2   4   5   7   8   9  10  14  16  17  18  19  21  22  23  24\n",
      "  25  26  27  28  29  31  32  33  34  35  36  37  38  39  40  41  42  43\n",
      "  44  45  47  49  50  51  53  54  55  56  57  58  59  60  61  62  63  65\n",
      "  66  67  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84\n",
      "  86  87  88  90  92  93  97  99 100 103 105 106 107 108 112 113 114 115\n",
      " 116 117 118 119 121 122 123 124 126 127 129 130 131 132 133 134 135 136\n",
      " 137 138 139 140 141 142 143 144 145 146 147 148]\n",
      "[  2   3   6   7   8   9  10  11  12  13  14  15  16  18  19  20  21  22\n",
      "  24  25  26  27  29  30  33  36  37  39  40  43  44  45  46  47  48  49\n",
      "  50  51  52  54  56  58  59  60  61  62  63  64  66  67  68  69  70  71\n",
      "  72  73  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
      "  92  93  94  95  96  97  98  99 100 101 102 103 104 106 107 108 109 110\n",
      " 111 112 114 115 116 117 119 120 121 122 123 125 126 127 128 130 132 133\n",
      " 134 135 136 137 140 141 142 144 145 146 147 149]\n",
      "[  0   1   2   3   4   5   6   7   8  10  11  12  13  15  16  17  18  20\n",
      "  22  23  24  26  27  28  30  31  32  33  34  35  37  38  40  41  42  43\n",
      "  44  45  46  48  50  51  52  53  54  55  56  57  59  60  61  62  63  64\n",
      "  65  66  68  69  71  73  74  75  76  78  80  83  84  85  86  89  90  91\n",
      "  92  93  94  95  96  97  98 100 101 102 104 105 106 107 108 109 110 111\n",
      " 112 113 114 116 118 119 120 121 123 124 125 126 127 128 129 131 132 133\n",
      " 134 135 137 138 139 141 143 144 146 147 148 149]\n",
      "Scores: [0.9666666666666667, 0.8666666666666667, 1.0, 1.0, 0.9333333333333333]\n",
      "Mean accuracy: 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Define KFold\n",
    "cv = 5\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=0)\n",
    "\n",
    "# Store scores\n",
    "scores = []\n",
    "print(f\"X : \", X)\n",
    "# KFold loop\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    # Split data\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    print(train_idx)\n",
    "    # Train estimator\n",
    "    clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Test estimator\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    scores.append(acc)\n",
    "\n",
    "print(\"Scores:\", scores)\n",
    "print(\"Mean accuracy:\", np.mean(scores))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
